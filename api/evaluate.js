// Vercel Serverless Function: /api/evaluate

const METRICS = [
  { id: 'medical_necessity', name: 'Medical Necessity Argumentation', weight: 10, description: 'Strength of clinical justification for the requested treatment/service' },
  { id: 'clinical_evidence', name: 'Clinical Evidence & Guideline Citation', weight: 9, description: 'References to peer-reviewed literature, NCCN, UpToDate, clinical guidelines' },
  { id: 'plan_language', name: 'Plan Language Reference', weight: 8, description: "Cites the insurer's own coverage criteria, plan documents, or formulary language" },
  { id: 'legal_regulatory', name: 'Legal/Regulatory Compliance', weight: 8, description: 'ERISA, ACA, state mandate references, regulatory framework accuracy' },
  { id: 'denial_rebuttal', name: 'Denial Reason Rebuttal', weight: 10, description: 'Directly and specifically addresses the stated denial rationale point by point' },
  { id: 'patient_specific', name: 'Patient-Specific Clinical Detail', weight: 7, description: 'Incorporates individual medical history, comorbidities, prior treatments' },
  { id: 'structural_completeness', name: 'Structural Completeness', weight: 6, description: 'Proper format: header, timeline, narrative, request, escalation path' },
  { id: 'persuasive_tone', name: 'Persuasive Tone Calibration', weight: 7, description: 'Assertive but professional tone; not adversarial or obsequious' },
  { id: 'actionable_request', name: 'Actionable Request Clarity', weight: 7, description: 'Unambiguous ask: overturn, peer-to-peer review, external review, expedited review' },
  { id: 'supporting_docs', name: 'Supporting Documentation References', weight: 6, description: 'Calls out attached records, letters of medical necessity, chart notes' },
  { id: 'urgency_timeline', name: 'Urgency & Timeline Communication', weight: 8, description: 'Deadlines, clinical consequences of delay, time-sensitive treatment needs' },
  { id: 'precedent_history', name: 'Precedent & Prior Auth History', weight: 6, description: 'Cites relevant prior authorizations, comparable approvals, or case law' },
  { id: 'readability', name: 'Readability & Clarity', weight: 5, description: 'Accessible to non-clinical reviewers while maintaining medical accuracy' },
  { id: 'required_elements', name: 'Completeness of Required Elements', weight: 7, description: 'All regulatory appeal requirements met (member ID, dates, provider info, etc.)' },
  { id: 'overall_effectiveness', name: 'Overall Predicted Effectiveness', weight: 10, description: 'Composite likelihood of appeal overturn based on all factors' },
];

function buildEvaluationPrompt(originalPrompt, outputs) {
  const metricsDescription = METRICS.map((m, i) =>
    `${i + 1}. **${m.name}** (ID: ${m.id}, Weight: ${m.weight}/10): ${m.description}`
  ).join('\n');

  const outputsText = outputs.map((o, i) =>
    `--- OUTPUT ${i + 1}: ${o.model} ---\n${o.text}\n--- END OUTPUT ${i + 1} ---`
  ).join('\n\n');

  return `You are an expert insurance appeals evaluator with deep knowledge of medical necessity determination, ERISA law, state insurance regulations, and clinical guideline-based argumentation.

You must evaluate the following insurance appeal letter outputs generated by different LLMs in response to the same prompt. Score each output on the 15 metrics below, using a scale of 1-10 for each metric.

## EVALUATION METRICS
${metricsDescription}

## ORIGINAL PROMPT
${originalPrompt}

## LLM OUTPUTS TO EVALUATE
${outputsText}

## INSTRUCTIONS
For each output, provide:
1. A score (1-10) for each of the 15 metrics
2. A brief justification (1-2 sentences) for each score
3. A weighted overall score calculated as: sum(metric_score * metric_weight) / sum(all_weights)

Return your evaluation as a JSON object with this exact structure:
{
  "evaluations": [
    {
      "model": "<model name>",
      "metrics": {
        "<metric_id>": { "score": <1-10>, "justification": "<brief explanation>" },
        ...
      },
      "weighted_overall": <calculated score>,
      "summary": "<2-3 sentence overall assessment>"
    }
  ],
  "recommendation": "<which output is strongest and why, 2-3 sentences>",
  "improvement_suggestions": "<key improvements any output could make, 2-3 sentences>"
}

Be rigorous, fair, and specific. Do not show bias toward any particular model. Evaluate purely on output quality.`;
}

async function callClaude(prompt, apiKey) {
  const res = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', 'x-api-key': apiKey, 'anthropic-version': '2023-06-01' },
    body: JSON.stringify({ model: 'claude-opus-4-6', max_tokens: 8192, messages: [{ role: 'user', content: prompt }] }),
  });
  if (!res.ok) throw new Error(`Claude API error (${res.status}): ${await res.text()}`);
  const data = await res.json();
  return data.content.map(c => c.text || '').join('');
}

async function callGPT(prompt, apiKey) {
  const res = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${apiKey}` },
    body: JSON.stringify({ model: 'gpt-5', messages: [{ role: 'user', content: prompt }], max_tokens: 8192 }),
  });
  if (!res.ok) throw new Error(`OpenAI API error (${res.status}): ${await res.text()}`);
  const data = await res.json();
  return data.choices[0].message.content;
}

async function callGLM(prompt, apiKey) {
  const res = await fetch('https://open.bigmodel.cn/api/paas/v4/chat/completions', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', 'Authorization': `Bearer ${apiKey}` },
    body: JSON.stringify({ model: 'glm-4-plus', messages: [{ role: 'user', content: prompt }], max_tokens: 8192 }),
  });
  if (!res.ok) throw new Error(`GLM API error (${res.status}): ${await res.text()}`);
  const data = await res.json();
  return data.choices[0].message.content;
}

export default async function handler(req, res) {
  if (req.method !== 'POST') return res.status(405).json({ error: 'Method not allowed' });

  const { originalPrompt, outputs, judgeModel, apiKeys, envKeys } = req.body;
  if (!originalPrompt || !outputs?.length) {
    return res.status(400).json({ error: 'Original prompt and outputs are required' });
  }

  const evalPrompt = buildEvaluationPrompt(originalPrompt, outputs);

  try {
    let responseText;
    const keyName = judgeModel.toUpperCase().replace(/[^A-Z0-9]/g, '_');
    const key = apiKeys?.[judgeModel] || (envKeys ? process.env[`${keyName}_API_KEY`] : null);

    if (judgeModel === 'claude-opus-4.6') {
      if (!key) throw new Error('Claude API key not provided for judge');
      responseText = await callClaude(evalPrompt, key);
    } else if (judgeModel === 'gpt-5') {
      if (!key) throw new Error('OpenAI API key not provided for judge');
      responseText = await callGPT(evalPrompt, key);
    } else if (judgeModel === 'glm-5') {
      if (!key) throw new Error('GLM API key not provided for judge');
      responseText = await callGLM(evalPrompt, key);
    } else {
      throw new Error(`Unknown judge model: ${judgeModel}`);
    }

    let parsed;
    try {
      const jsonMatch = responseText.match(/```(?:json)?\s*([\s\S]*?)```/);
      const jsonStr = jsonMatch ? jsonMatch[1].trim() : responseText.trim();
      parsed = JSON.parse(jsonStr);
    } catch {
      const start = responseText.indexOf('{');
      const end = responseText.lastIndexOf('}');
      if (start !== -1 && end !== -1) {
        parsed = JSON.parse(responseText.substring(start, end + 1));
      } else {
        throw new Error('Could not parse evaluation response as JSON');
      }
    }

    res.json({ evaluation: parsed });
  } catch (err) {
    res.status(500).json({ error: err.message });
  }
}
